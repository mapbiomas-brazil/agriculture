{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0J_1HmkCGix"
      },
      "source": [
        "# Model Prediction\n",
        "\n",
        "We use Google Drive to share the data used for training the model and the model already trained. To add a shortcut to the location of the data in your Google Drive, follow these steps:    \n",
        "\n",
        "* Go to https://drive.google.com/drive/folders/1EPfe4G6Y32c6eTrM6gnTjxr7KUlrGwAt?usp=sharing\n",
        "* Click in \"MAPBIOMAS-PUBLIC\" &#8594; \"Add Shortcut to Drive\" &#8594; \"My Drive\" &#8594; \"ADD SHORTCUT\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igw_HamJhs1n"
      },
      "source": [
        "# Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHxDrtgiA5yp"
      },
      "source": [
        "#### Install libraries and mount drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKu6MAyTBHJp",
        "outputId": "8a02299d-d897-47e9-c886-98fb8857a45e"
      },
      "outputs": [],
      "source": [
        "!pip install retry\n",
        "\n",
        "import logging\n",
        "import json\n",
        "from retry import retry\n",
        "import ee\n",
        "import multiprocessing\n",
        "import os\n",
        "import logging\n",
        "from google.auth.transport.requests import AuthorizedSession\n",
        "from google.oauth2 import service_account\n",
        "from google.colab import drive, runtime\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnPOOK-Tno3E",
        "outputId": "8c402f0a-e1f9-4406-c878-d65d714f0e6c"
      },
      "outputs": [],
      "source": [
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27XA_E__BkGA"
      },
      "source": [
        "\n",
        "# <span style=\"color:red\">Google Earth Engine REST API functions</span>\n",
        "\n",
        "This is an alternative way to download small images from Google Earth Engine without creating *tasks*, which requires a more advanced knowledge of the platform to implement. \n",
        "\n",
        "If you choose to download the images in the traditional way to a folder on Google Drive, skip the grouped cells here and change `PREDICT_INPUT_DIR` in settings to your folder directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsKKoViV_ayM"
      },
      "source": [
        "#### Altenticar com o GEE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_UtxtaEBOYu",
        "outputId": "743ba626-c1ec-4cc1-b5d7-f1154774ccb9"
      },
      "outputs": [],
      "source": [
        "ee.Authenticate()\n",
        "\n",
        "ee.Initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JI87NqIts6_7"
      },
      "outputs": [],
      "source": [
        "KEY = \"/content/drive/MyDrive/your_key.json\"\n",
        "EMAIL = \"your_email@google.com\"\n",
        "\n",
        "PIXEL_SIZE = 30\n",
        "CHIP_SIZE = 1024\n",
        "BASE_PROJ = ee.Projection(\"EPSG:4326\")\n",
        "\n",
        "EXPORT_TO_TIFF = True\n",
        "\n",
        "ASSETS_PATH = \"users/your_user/path\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqZVT2cI_tT7"
      },
      "source": [
        "#### Conect with GCLoud json key\n",
        "\n",
        "Attention! You will need a Google Cloud *service account* to follow this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djxO080qBXAx",
        "outputId": "47bd8aad-8600-427f-a584-2ce382240b89"
      },
      "outputs": [],
      "source": [
        "!gcloud config set account {EMAIL}\n",
        "!gcloud auth activate-service-account --key-file '{KEY}'\n",
        "\n",
        "credentials = service_account.Credentials.from_service_account_file(KEY)\n",
        "scoped_credentials = credentials.with_scopes(\n",
        "    [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
        ")\n",
        "\n",
        "session = AuthorizedSession(scoped_credentials)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IJEjDrPewpP",
        "outputId": "95ba8560-3cef-46b9-aa25-7fe788c98496"
      },
      "outputs": [],
      "source": [
        "if True:  # Testing API connection\n",
        "    response = session.get(\n",
        "        \"https://earthengine.googleapis.com/v1/projects/earthengine-public/assets/LANDSAT\"\n",
        "    )\n",
        "    print(json.loads(response.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HOe4CtzUZ_h"
      },
      "outputs": [],
      "source": [
        "url = \"https://earthengine-highvolume.googleapis.com/v1/projects/earthengine-public/image:computePixels\"\n",
        "os.makedirs(\"results/\", exist_ok=True)\n",
        "os.makedirs(\"classifications/\", exist_ok=True)\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "fh = logging.FileHandler(\"app.log\")\n",
        "fh.setLevel(logging.INFO)\n",
        "logger.addHandler(fh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAD_HX4ZRFWz"
      },
      "outputs": [],
      "source": [
        "# Landsat Collection 2\n",
        "def mask_clouds(image):\n",
        "    dilatedCloud = (1 << 1)\n",
        "    cloud = (1 << 3)\n",
        "    cloudShadow = (1 << 4)\n",
        "    snow = (1 << 5)\n",
        "    qa = image.select('BQA')\n",
        "    mask = qa.bitwiseAnd(dilatedCloud)\\\n",
        "    .Or(qa.bitwiseAnd(cloud))\\\n",
        "    .Or(qa.bitwiseAnd(cloudShadow))\\\n",
        "    .Or(qa.bitwiseAnd(snow))\n",
        "    \n",
        "    mask2 = image.mask().reduce(ee.Reducer.min())\n",
        "    return image.updateMask(mask.Not()).updateMask(mask2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2XSXlFWOndm"
      },
      "outputs": [],
      "source": [
        "properties_dict = ee.Dictionary({\n",
        "  'LANDSAT_4': {\n",
        "    'C1': {\n",
        "      'TOA': {\n",
        "        'bands': ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'BQA'],\n",
        "        'newBandNames': ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'TIR1', 'SWIR2', 'BQA'],\n",
        "        'scaleFactors': [1, 1, 1, 1, 1, 1, 1, 1],\n",
        "        'offset': [0, 0, 0, 0, 0, 0, 0, 0]\n",
        "      },\n",
        "    },\n",
        "    'C2': {\n",
        "      'TOA': {\n",
        "        'bands': ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'QA_PIXEL'],\n",
        "        'newBandNames': ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'TIR1', 'SWIR2', 'BQA'],\n",
        "        'scaleFactors': [1, 1, 1, 1, 1, 1, 1, 1],\n",
        "        'offset': [0, 0, 0, 0, 0, 0, 0, 0]\n",
        "      },\n",
        "      'SR': {\n",
        "        'bands': ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7', 'QA_PIXEL'],\n",
        "        'newBandNames': ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2', 'BQA'],\n",
        "        'scaleFactors': [0.0000275, 0.0000275, 0.0000275, 0.0000275, 0.0000275, 0.0000275, 1],\n",
        "        'offset': [-0.2, -0.2, -0.2, -0.2, -0.2 ,-0.2 , 0]\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  'LANDSAT_5': {\n",
        "    'C1': {\n",
        "      'TOA': {\n",
        "        'bands': ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'BQA'],\n",
        "        'newBandNames': ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'TIR1', 'SWIR2', 'BQA'],\n",
        "        'scaleFactors': [1, 1, 1, 1, 1, 1, 1, 1],\n",
        "        'offset': [0, 0, 0, 0, 0, 0, 0, 0]\n",
        "      },\n",
        "    },\n",
        "    'C2': {\n",
        "      'TOA': {\n",
        "        'bands': ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'QA_PIXEL'],\n",
        "        'newBandNames': ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'TIR1', 'SWIR2', 'BQA'],\n",
        "        'scaleFactors': [1, 1, 1, 1, 1, 1, 1, 1],\n",
        "        'offset': [0, 0, 0, 0, 0, 0, 0, 0]\n",
        "      },\n",
        "      'SR': {\n",
        "        'bands': ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7', 'QA_PIXEL'],\n",
        "        'newBandNames': ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2', 'BQA'],\n",
        "        'scaleFactors': [0.0000275, 0.0000275, 0.0000275, 0.0000275, 0.0000275, 0.0000275, 1],\n",
        "        'offset': [-0.2, -0.2, -0.2, -0.2, -0.2 ,-0.2 , 0]\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  'LANDSAT_7': {\n",
        "    'C1': {\n",
        "      'TOA': {\n",
        "        'bands': ['B1', 'B2', 'B3', 'B4', 'B5', 'B7', 'BQA'],\n",
        "        'newBandNames': ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2', 'BQA'],\n",
        "        'scaleFactors': [1, 1, 1, 1, 1, 1, 1],\n",
        "        'offset': [0, 0, 0, 0, 0, 0, 0]\n",
        "      },\n",
        "    },\n",
        "    'C2': {\n",
        "      'TOA': {\n",
        "        'bands': ['B1', 'B2', 'B3', 'B4', 'B5', 'B7', 'QA_PIXEL'],\n",
        "        'newBandNames': ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2', 'BQA'],\n",
        "        'scaleFactors': [1, 1, 1, 1, 1, 1, 1],\n",
        "        'offset': [0, 0, 0, 0, 0, 0, 0]\n",
        "      },\n",
        "      'SR': {\n",
        "        'bands': ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7', 'QA_PIXEL'],\n",
        "        'newBandNames': ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2', 'BQA'],\n",
        "        'scaleFactors': [0.0000275, 0.0000275, 0.0000275, 0.0000275, 0.0000275, 0.0000275, 1],\n",
        "        'offset': [-0.2, -0.2, -0.2, -0.2, -0.2 ,-0.2 , 0]\n",
        "      }\n",
        "    }\n",
        "  },  \n",
        "  'LANDSAT_8': {\n",
        "    'C1': {\n",
        "      'TOA': {\n",
        "        'bands': ['B2', 'B3','B4', 'B5', 'B6', 'B7', 'B10', 'B11', 'BQA'],\n",
        "        'newBandNames': ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2', 'TIR1', 'TIR2', 'BQA'],\n",
        "        'scaleFactors': [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "        'offset': [0, 0, 0, 0, 0, 0, 0, 0 ,0]\n",
        "      },\n",
        "    },\n",
        "    'C2': {\n",
        "      'TOA': {\n",
        "        'bands': ['B2', 'B3','B4', 'B5', 'B6', 'B7', 'B10', 'B11', 'QA_PIXEL'],\n",
        "        'newBandNames': ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2', 'TIR1', 'TIR2', 'BQA'],\n",
        "        'scaleFactors': [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "        'offset': [0, 0, 0, 0, 0, 0, 0, 0 ,0]\n",
        "      },\n",
        "      'SR': {\n",
        "        'bands': ['SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7', 'QA_PIXEL'],\n",
        "        'newBandNames': ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2', 'BQA'],\n",
        "        'scaleFactors': [0.0000275, 0.0000275, 0.0000275, 0.0000275, 0.0000275, 0.0000275, 1],\n",
        "        'offset': [-0.2, -0.2, -0.2, -0.2, -0.2 ,-0.2 , 0]\n",
        "      }\n",
        "    }\n",
        "  },  \n",
        "  'LANDSAT_9': {\n",
        "    'C2': {\n",
        "      'TOA': {\n",
        "        'bands': ['B2', 'B3','B4', 'B5', 'B6', 'B7', 'B10', 'B11', 'QA_PIXEL'],\n",
        "        'newBandNames': ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2', 'TIR1', 'TIR2', 'BQA'],\n",
        "        'scaleFactors': [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "        'offset': [0, 0, 0, 0, 0, 0, 0, 0 ,0]\n",
        "      },\n",
        "      'SR': {\n",
        "        'bands': ['SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7', 'QA_PIXEL'],\n",
        "        'newBandNames': ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2', 'BQA'],\n",
        "        'scaleFactors': [0.0000275, 0.0000275, 0.0000275, 0.0000275, 0.0000275, 0.0000275, 1],\n",
        "        'offset': [-0.2, -0.2, -0.2, -0.2, -0.2 ,-0.2 , 0]\n",
        "      }\n",
        "    }\n",
        "  },  \n",
        "  'Sentinel-2A': {\n",
        "    'C1': {\n",
        "      'TOA-SR': {\n",
        "        'bands': ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12', 'QA60'],\n",
        "        'newBandNames': ['BLUE', 'GREEN', 'RED', 'RE1', 'RE2', 'RE3', 'NIR', 'RE4', 'SWIR1', 'SWIR2', 'BQA'],\n",
        "        'scaleFactors': [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 1],\n",
        "        'offset': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "      }\n",
        "    }\n",
        "  },  \n",
        "  'Sentinel-2B': {\n",
        "    'C1': {\n",
        "      'TOA-SR': {\n",
        "        'bands': ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12', 'QA60'],\n",
        "        'newBandNames': ['BLUE', 'GREEN', 'RED', 'RE1', 'RE2', 'RE3', 'NIR', 'RE4', 'SWIR1', 'SWIR2', 'BQA'],\n",
        "        'scaleFactors': [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 1],\n",
        "        'offset': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  'MODIS': {\n",
        "    'C1': {\n",
        "      'SR': {\n",
        "        'bands': ['sur_refl_b03', 'sur_refl_b04', 'sur_refl_b01', 'sur_refl_b02', 'sur_refl_b06', 'sur_refl_b07', 'StateQA'],\n",
        "        'newBandNames': ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2', 'BQA'],\n",
        "        'scaleFactors': [1, 1, 1, 1, 1, 1, 1],\n",
        "        'offset': [0, 0, 0, 0, 0, 0, 0]\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "});\n",
        "\n",
        "def get_landsat_properties(landsatImage):\n",
        "    landsatProperties =  landsatImage.toDictionary(landsatImage.propertyNames())\n",
        "  \n",
        "    return landsatProperties\\\n",
        "        .set('SATELLITE_NAME', landsatProperties.getString('SPACECRAFT_ID'))\\\n",
        "        .set('CLOUD_COVER', landsatProperties.getNumber('CLOUD_COVER'))\\\n",
        "        .set('COLLECTION', 'C2')\\\n",
        "        .set('REFLECTANCE', ee.Algorithms.If(\n",
        "                                            landsatImage.bandNames().containsAll(['SR_B2', 'SR_B3', 'SR_B4']),\n",
        "                                            'SR',\n",
        "                                            'TOA'\n",
        "                                            ))\n",
        "\n",
        "\n",
        "def get_sentinel_properties(sentinelImage):\n",
        "    sentinelProperties = sentinelImage.toDictionary(sentinelImage.propertyNames())\n",
        "  \n",
        "    return sentinelProperties\\\n",
        "        .set('SATELLITE_NAME', sentinelProperties.getString('SPACECRAFT_NAME'))\\\n",
        "        .set('CLOUD_COVER', sentinelProperties.getNumber('CLOUDY_PIXEL_PERCENTAGE'))\\\n",
        "        .set('COLLECTION', 'C1')\\\n",
        "        .set('REFLECTANCE', 'TOA-SR')\n",
        "\n",
        "\n",
        "def get_MODIS_properties(modisImage):\n",
        "    return ee.Dictionary({\n",
        "        'SATELLITE_NAME': 'MODIS',\n",
        "        'COLLECTION': 'C1',\n",
        "        'REFLECTANCE': 'SR'\n",
        "    })\n",
        "\n",
        "\n",
        "def get_properties(image):\n",
        "    return ee.Dictionary(ee.Algorithms.If(ee.List(['LANDSAT_5', 'LANDSAT_7', 'LANDSAT_8', 'LANDSAT_9']).containsAll([image.get('SPACECRAFT_ID')]), \n",
        "                        get_landsat_properties(image), \n",
        "                        ee.Algorithms.If(ee.List(['Sentinel-2A', 'Sentinel-2B']).containsAll([image.get('SPACECRAFT_NAME')]),\n",
        "                            get_sentinel_properties(image),\n",
        "                            get_MODIS_properties(image)\n",
        "                            )\n",
        "                        ))\n",
        "\n",
        "\n",
        "def standardize_image(image):\n",
        "    imageProperties = get_properties(image)\n",
        "\n",
        "    properties = ee.Dictionary(ee.Dictionary(ee.Dictionary(\n",
        "                        properties_dict\\\n",
        "                        .get(imageProperties.get('SATELLITE_NAME')))\\\n",
        "                        .get(imageProperties.get('COLLECTION')))\\\n",
        "                        .get(imageProperties.get('REFLECTANCE')))\\\n",
        "\n",
        "    renamedImage = image.select(properties.get('bands'), properties.get('newBandNames'))\n",
        "\n",
        "    reescaledImage = renamedImage.multiply(ee.Number(ee.List(properties.get('scaleFactors')))).add(ee.Number(ee.List(properties.get('offset'))))\n",
        "    \n",
        "    return ee.Image(reescaledImage.setMulti(imageProperties))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Install GDAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMJ9PdumYYjn",
        "outputId": "f90a0bf8-976f-4a87-a58d-23845cd35fd0"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install gdal-bin -y\n",
        "!sudo add-apt-repository ppa:ubuntugis/ppa -y\n",
        "!sudo apt-get update -y\n",
        "!pip install retry rasterio\n",
        "import rasterio as rio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkjAEgd3BljD"
      },
      "outputs": [],
      "source": [
        "def prepareImages(assets_path):\n",
        "    \"\"\"Generates a list of files to be downloaded.\"\"\"\n",
        "    points = (\n",
        "        ee.FeatureCollection(assets_path)\n",
        "        .map(lambda ft: ft.geometry().centroid())\n",
        "        .aggregate_array(\".geo\")\n",
        "        .getInfo()\n",
        "    )\n",
        "    squares = [\n",
        "        ee.Geometry.Point(ft[\"coordinates\"]).buffer(PIXEL_SIZE * CHIP_SIZE / 2).bounds()\n",
        "        for ft in points\n",
        "    ]\n",
        "\n",
        "    return list(enumerate(squares))\n",
        "\n",
        "\n",
        "@retry(tries=10, delay=5, backoff=3)\n",
        "def downloadEachImage (args):\n",
        "\n",
        "  \"\"\"Download each file of list.\"\"\"\n",
        "  \n",
        "  start = str(years[x])+'-01-01' \n",
        "  end = str(years[x])+'-12-31' \n",
        "  cloud_cover = 80\n",
        "  \n",
        "  index = args[0]\n",
        "  region = args[1]\n",
        "  \n",
        "  filters = ee.Filter.And(\\\n",
        "      ee.Filter.bounds(region),\\\n",
        "      ee.Filter.date(start, end),\\\n",
        "      ee.Filter.lt('CLOUD_COVER', cloud_cover)\n",
        "  )\n",
        "  \n",
        "  ls5 = ee.ImageCollection(\"LANDSAT/LT05/C02/T1_TOA\").filter(filters)\n",
        "  ls7 = ee.ImageCollection(\"LANDSAT/LE07/C02/T1_TOA\").filter(filters)\n",
        "  ls8 = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_TOA\").filter(filters)\n",
        "  ls9 = ee.ImageCollection(\"LANDSAT/LC09/C02/T1_TOA\").filter(filters)\n",
        "  \n",
        "  expression = (\n",
        "      ls5.merge(ls7).merge(ls8).merge(ls9)\\\n",
        "      .map(standardize_image)\\\n",
        "      .map(mask_clouds)\\\n",
        "      .select([\"RED\", \"NIR\", \"SWIR1\"])\\ \n",
        "      .median()\\\n",
        "      .reproject(BASE_PROJ.crs(), None, PIXEL_SIZE)\n",
        "  )\n",
        "  # Get scales out of the transform.\n",
        "  proj = ee.Projection(\"EPSG:4326\").atScale(PIXEL_SIZE).getInfo()\n",
        "  scale_y = -proj[\"transform\"][0]\n",
        "  scale_x = proj[\"transform\"][4]\n",
        "\n",
        "  # Extraction parameters\n",
        "  listCoords = ee.Array.cat(region.coordinates(), 1).getInfo()\n",
        "\n",
        "  # coords\n",
        "  xMin = listCoords[0][0]\n",
        "  yMax = listCoords[2][1]\n",
        "  coords = [xMin, yMax]\n",
        "\n",
        "  grid = {\n",
        "        \"dimensions\": {\"width\": CHIP_SIZE, \"height\": CHIP_SIZE},\n",
        "        \"affineTransform\": {\n",
        "            \"scaleX\": scale_x,\n",
        "            \"scaleY\": scale_y,\n",
        "            \"translateX\": coords[0],\n",
        "            \"translateY\": coords[1],\n",
        "        },\n",
        "        \"crsCode\": \"EPSG:4326\",\n",
        "    }\n",
        "\n",
        "  response = session.post(\n",
        "        url=url,\n",
        "        data=json.dumps(\n",
        "            {\n",
        "                \"expression\": ee.serializer.encode(expression),\n",
        "                \"fileFormat\": \"GEO_TIFF\" if EXPORT_TO_TIFF else \"NPY\",\n",
        "                \"bandIds\": expression.bandNames().getInfo(),\n",
        "                \"grid\": grid,\n",
        "            }\n",
        "        ),\n",
        "    )\n",
        "\n",
        "  try:\n",
        "        error = json.loads(response.content)\n",
        "        logger = logging.getLogger()\n",
        "        logger.error(str(json.loads(response.content)))\n",
        "        raise Exception(\"run again\")\n",
        "  except UnicodeDecodeError:\n",
        "        if EXPORT_TO_TIFF:\n",
        "            with rio.io.MemoryFile(response.content) as memfile:\n",
        "                with memfile.open() as input:\n",
        "                    with rio.open(\n",
        "                        f\"results/mosaic_{index}_{str(years[x])}.tiff\",\n",
        "                        \"w\",\n",
        "                        **dict(\n",
        "                            input.meta,\n",
        "                            compress=\"DEFLATE\",\n",
        "                            nodata=-1,\n",
        "                        ),\n",
        "                    ) as output:\n",
        "                        output.write(input.read())\n",
        "        else:\n",
        "            with open(f\"results/mosaic_{index}.npy\", \"wb\") as file:\n",
        "                file.write(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEun23heBmQ2"
      },
      "source": [
        "## Requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRqSzHEyiPZC"
      },
      "outputs": [],
      "source": [
        "years = list(range(1985,2023)) # ------------> choose the range of years to download images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUV1w3Hvrb-U"
      },
      "outputs": [],
      "source": [
        "for x in range(0, len(years)):\n",
        "  items = prepareImages(ASSETS_PATH)\n",
        "\n",
        "  with multiprocessing.Pool() as pool:\n",
        "      count = 0\n",
        "      for _ in pool.imap_unordered(\n",
        "          downloadEachImage, items, chunksize=max(len(items) // os.cpu_count(), 1)\n",
        "      ):\n",
        "          pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWdjJUCdt2ei",
        "outputId": "99d1a4ae-dd34-417c-b0bc-bad0823c491d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.1G\tresults\n"
          ]
        }
      ],
      "source": [
        "!du -h results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIyZKfiA5387"
      },
      "source": [
        "# Settings for Model and Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3fRpqzU59z3"
      },
      "source": [
        "### Install GDAL and environment Setting "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxZvKNPqQczc",
        "outputId": "3e976ac0-c7cf-46b8-eecb-a733b5b66472"
      },
      "outputs": [],
      "source": [
        "#Step 1\n",
        "!apt-get update\n",
        "#Step 2\n",
        "!apt-get install libgdal-dev -y\n",
        "#Step 3\n",
        "!apt-get install python-gdal -y\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "seed = 426\n",
        "random.seed(seed)\n",
        "np.random.seed = seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGGqcMoG6GSF"
      },
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQw_tglOQlmj"
      },
      "outputs": [],
      "source": [
        "CHIP_SIZE = 256\n",
        "CHANNELS = 3\n",
        "LABELS = [0, 1]\n",
        "SPATIAL_SCALE = 30\n",
        "PROJECTION = 3857\n",
        "REPROJECT = False\n",
        "\n",
        "\n",
        "# Build Datasets\n",
        "GRIDS = 2\n",
        "ROTATE = True\n",
        "FLIP = True\n",
        "\n",
        "PUBLIC_ROOT_DIR =\"/content/drive/My Drive/MAPBIOMAS-PUBLIC/collection_8/oil_palm\" \n",
        "DATASET_DIR = None\n",
        "\n",
        "# Train model\n",
        "# in case of error during training, decrease the value of TRAIN_BATCH_SIZE\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "TRAIN_EPOCHS = 100\n",
        "\n",
        "# Predict images\n",
        "PREDICT_INPUT_DIR = \"/content/results\" # <-- CHANGE THIS TO YOUR DRIVE SAMPLES FOLDER PATH IF YOU SKIPPED THE \"Google Earth Engine REST API functions\" BLOCK. \n",
        "PREDICT_CHIP_SIZE = 256\n",
        "PREDICT_GRIDS = 3\n",
        "PREDICT_BATCH_SIZE = 1\n",
        "PREDICT_OUPUT_DIR = \"/content/classifications\"\n",
        "\n",
        "# Load trained model\n",
        "MODEL_DIR = f\"{PUBLIC_ROOT_DIR}/logs\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIUJqb6s6LFC"
      },
      "source": [
        "## Image Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS9aROwLE1LN"
      },
      "outputs": [],
      "source": [
        "# -*- encoding=UTF-8-*-\n",
        "\n",
        "import gc\n",
        "import os\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import psutil\n",
        "from osgeo import gdal, osr\n",
        "from skimage.transform import resize, rotate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_file(path, resizeTo=None, norm=False):\n",
        "  \n",
        "    original_source = gdal.Open(path)\n",
        "\n",
        "    if REPROJECT:\n",
        "        new_source = reproject_dataset(original_source,\n",
        "                                    pixel_spacing=SPATIAL_SCALE,\n",
        "                                    epsg_to=PROJECTION)\n",
        "    else:\n",
        "        dataSource = original_source\n",
        "\n",
        "    if not dataSource is None:\n",
        "        bands = []\n",
        "        for index in range(1, dataSource.RasterCount + 1):\n",
        "            band = dataSource.GetRasterBand(index).ReadAsArray()\n",
        "            if norm:\n",
        "                normalized_band = normalize(band)\n",
        "                bands.append(normalized_band)\n",
        "            else:\n",
        "                bands.append(band)\n",
        "\n",
        "        image = np.dstack(bands)\n",
        "\n",
        "        return image\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def reproject_dataset ( dataset, pixel_spacing=30., epsg_to=3857 ):\n",
        "    \"\"\"\n",
        "    A sample function to reproject and resample a GDAL dataset from within \n",
        "    Python. The idea here is to reproject from one system to another, as well\n",
        "    as to change the pixel size. The procedure is slightly long-winded, but\n",
        "    goes like this:\n",
        "    \n",
        "    1. Set up the two Spatial Reference systems.\n",
        "    2. Open the original dataset, and get the geotransform\n",
        "    3. Calculate bounds of new geotransform by projecting the UL corners \n",
        "    4. Calculate the number of pixels with the new projection & spacing\n",
        "    5. Create an in-memory raster dataset\n",
        "    6. Perform the projection\n",
        "    \"\"\"\n",
        "    # We now open the dataset\n",
        "    g = gdal.Open ( dataset )\n",
        "\n",
        "    # Define the UK OSNG, see <http://spatialreference.org/ref/epsg/27700/>\n",
        "    osng = osr.SpatialReference ()\n",
        "    osng.ImportFromEPSG ( epsg_to )\n",
        "\n",
        "    wkt = g.GetProjection()\n",
        "    wgs84 = osr.SpatialReference ()\n",
        "    wgs84.ImportFromWkt(wkt)\n",
        "\n",
        "    tx = osr.CoordinateTransformation ( wgs84, osng )\n",
        "    # Up to here, all  the projection have been defined, as well as a \n",
        "    # transformation from the from to the  to :)\n",
        "\n",
        "    # Get the Geotransform vector\n",
        "    geo_t = g.GetGeoTransform ()\n",
        "    x_size = g.RasterXSize # Raster xsize\n",
        "    y_size = g.RasterYSize # Raster ysize\n",
        "    # Work out the boundaries of the new dataset in the target projection\n",
        "    (ulx, uly, ulz ) = tx.TransformPoint( geo_t[0], geo_t[3])\n",
        "    (lrx, lry, lrz ) = tx.TransformPoint( geo_t[0] + geo_t[1]*x_size, \\\n",
        "                                          geo_t[3] + geo_t[5]*y_size )\n",
        "    # See how using 27700 and WGS84 introduces a z-value!\n",
        "    # Now, we create an in-memory raster\n",
        "    mem_drv = gdal.GetDriverByName( 'MEM' )\n",
        "    # The size of the raster is given the new projection and pixel spacing\n",
        "    # Using the values we calculated above. Also, setting it to store one band\n",
        "    # and to use Float32 data type.\n",
        "    dest = mem_drv.Create('', int((lrx - ulx)/pixel_spacing), \\\n",
        "            int((uly - lry)/pixel_spacing), g.RasterCount, gdal.GDT_Float32)\n",
        "    # Calculate the new geotransform\n",
        "    new_geo = ( ulx, pixel_spacing, geo_t[2], \\\n",
        "                uly, geo_t[4], -pixel_spacing )\n",
        "    # Set the geotransform\n",
        "    dest.SetGeoTransform( new_geo )\n",
        "    dest.SetProjection ( osng.ExportToWkt() )\n",
        "    # Perform the projection/resampling \n",
        "    res = gdal.ReprojectImage( g, dest, \\\n",
        "                wgs84.ExportToWkt(), osng.ExportToWkt(), \\\n",
        "                gdal.GRA_Bilinear )\n",
        "    return dest\n",
        "\n",
        "\n",
        "def normalize(image):\n",
        "    image_max = float(np.max(image))\n",
        "    image_min = float(np.min(image))\n",
        "    normalized = (image - image_min) / (image_max - image_min)\n",
        "\n",
        "    del image\n",
        "    gc.collect()\n",
        "    return normalized\n",
        "\n",
        "\n",
        "def get_rotate(image):\n",
        "    images = []\n",
        "    for rot in [90, 180, 270]:\n",
        "        image_rotate = rotate(image, rot, preserve_range=True)\n",
        "        images.append(image_rotate)\n",
        "\n",
        "    del image\n",
        "    gc.collect()\n",
        "    return images\n",
        "\n",
        "\n",
        "def get_flip(image):\n",
        "    horizontal_flip = image[:, ::-1]\n",
        "    vertical_flip = image[::-1, :]\n",
        "\n",
        "    del image\n",
        "    gc.collect()\n",
        "    return [horizontal_flip, vertical_flip]\n",
        "\n",
        "\n",
        "def load_dataset(dataset, read_only=False):\n",
        "    if read_only:\n",
        "        dataset = h5py.File(dataset, 'r')\n",
        "    else:\n",
        "        dataset = h5py.File(dataset, 'r+')\n",
        "    x_data = dataset[\"x\"]\n",
        "    y_data = dataset[\"y\"]\n",
        "    return dataset, x_data, y_data\n",
        "\n",
        "\n",
        "def make_dataset(filename, width, height, channels):\n",
        "    dataset = h5py.File(filename, 'w')\n",
        "    x_data = dataset.create_dataset(\"x\", (0, width, height, channels), 'f',\n",
        "                                    maxshape=(None, width, height, channels),\n",
        "                                    chunks=True\n",
        "                                    # , compress=\"gzip\"\n",
        "                                    )\n",
        "    y_data = dataset.create_dataset(\"y\", (0, width, height, 1), 'f',\n",
        "                                    maxshape=(None, width, height, 1),\n",
        "                                    chunks=True\n",
        "                                    # , compress=\"gzip\"\n",
        "                                    )\n",
        "    return dataset, x_data, y_data\n",
        "\n",
        "\n",
        "def get_available_memory():\n",
        "    return 100 - psutil.virtual_memory().percent\n",
        "\n",
        "\n",
        "def chip_is_empty(chip):\n",
        "    labels_unique = np.unique(chip)\n",
        "    if 0 in labels_unique and len(labels_unique) == 1:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def generate_dataset(image_path, labels_path, train_path, validation_path, \n",
        "                     chip_size, channels, grids=1, rotate=False, flip=False):\n",
        "    image_data = load_file(image_path, norm=True)\n",
        "    image_labels = load_file(labels_path)\n",
        "\n",
        "    # resize labels\n",
        "    image_labels = resize(image_labels,\n",
        "                          (image_data.shape[0], image_data.shape[1]),\n",
        "                          preserve_range=True, anti_aliasing=True).astype(np.int8)\n",
        "\n",
        "    image = np.dstack([image_data, image_labels])\n",
        "\n",
        "    # pad image to avoid the loss of border samples\n",
        "    del image_data\n",
        "    del image_labels\n",
        "    gc.collect()\n",
        "\n",
        "    X_set = []\n",
        "    y_set = []\n",
        "    for step in get_grids(grids, chip_size):\n",
        "        for (x, y, window, dimension) in sliding_window(image,\n",
        "                                                        step[\"steps\"],\n",
        "                                                        step[\"chip_size\"],\n",
        "                                                        (\n",
        "                                                                chip_size,\n",
        "                                                                chip_size)):\n",
        "            \n",
        "            train = np.array(window[:, :, : channels], dtype=np.float16)\n",
        "            labels = np.array(window[:, :, -1:], dtype=np.int8)\n",
        "\n",
        "            if not chip_is_empty(labels):\n",
        "                raw_image = np.dstack([train, labels])\n",
        "                images_daugmentation = [raw_image]\n",
        "\n",
        "                if rotate:\n",
        "                    images_rotate = get_rotate(raw_image)\n",
        "                    images_daugmentation.extend(images_rotate)\n",
        "\n",
        "                if flip:\n",
        "                    images_flip = []\n",
        "                    for im in images_daugmentation:\n",
        "                        images_flip.extend(get_flip(im))\n",
        "                    images_daugmentation.extend(images_flip)\n",
        "\n",
        "                for i in images_daugmentation:\n",
        "                    new_train = np.array(i[:, :, :channels], dtype=np.float16)\n",
        "                    new_labels = np.array(i[:, :, -1:], dtype=np.int8)\n",
        "\n",
        "                    np.clip(new_labels, 0, None, out=new_labels)\n",
        "                    X_set.append(new_train)\n",
        "                    y_set.append(new_labels)\n",
        "        \n",
        "        print(\"\\nAll:\", len(X_set))\n",
        "\n",
        "        if len(X_set) >= 5:\n",
        "            X_train, X_val, y_train, y_val = train_test_split(X_set, y_set, test_size=0.25, random_state=1)\n",
        "            \n",
        "            print(\"X_train:\", len(X_train))\n",
        "            print(\"X_val:\", len(X_val))\n",
        "\n",
        "            save_dataset(X_train, y_train, train_path, chip_size, channels)\n",
        "            save_dataset(X_val, y_val, validation_path, chip_size, channels)\n",
        "\n",
        "\n",
        "def save_dataset(X, y, output_path, chip_size, channels):\n",
        "    if os.path.isfile(output_path):\n",
        "        dataset, x_data, y_data = load_dataset(output_path)\n",
        "    else:\n",
        "        dataset, x_data, y_data = make_dataset(output_path, chip_size,\n",
        "                                               chip_size, channels)\n",
        "\n",
        "    length = len(X)\n",
        "\n",
        "    x_data_size = x_data.len()\n",
        "    y_data_size = y_data.len()\n",
        "\n",
        "    x_data.resize((x_data_size + length, chip_size, chip_size, channels))\n",
        "    y_data.resize((y_data_size + length, chip_size, chip_size, 1))\n",
        "\n",
        "    print(\"Saving dataset...\")\n",
        "    \n",
        "    x_data[x_data_size:] = X\n",
        "    y_data[y_data_size:] = y\n",
        "    \n",
        "    dataset.close()\n",
        "    del x_data\n",
        "    del y_data\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "def sliding_window(image, step, chip_size, chip_resize):\n",
        "    # slide a chip across the image\n",
        "    step_cols = int(step[0])\n",
        "    step_rows = int(step[1])\n",
        "\n",
        "    cols = image.shape[1]\n",
        "    rows = image.shape[0]\n",
        "\n",
        "    chip_size_cols = chip_size[0]\n",
        "    chip_size_rows = chip_size[1]\n",
        "\n",
        "    chip_resize_cols = chip_resize[0]\n",
        "    chip_resize_rows = chip_resize[1]\n",
        "\n",
        "    for y in range(0, rows, step_rows):\n",
        "        for x in range(0, cols, step_cols):\n",
        "\n",
        "            origin_x = x\n",
        "            origin_y = y\n",
        "\n",
        "            if (origin_y + chip_size_rows) > rows:\n",
        "                origin_y = rows - chip_size_rows\n",
        "\n",
        "            if (origin_x + chip_size_cols) > cols:\n",
        "                origin_x = cols - chip_size_cols\n",
        "\n",
        "            chip = image[origin_y:origin_y + chip_size_rows,\n",
        "                   origin_x: origin_x + chip_size_cols]\n",
        "\n",
        "            original_shape = chip.shape\n",
        "\n",
        "            if chip.shape != (chip_resize_cols, chip_resize_rows):\n",
        "                chip = resize(chip,\n",
        "                              (chip_resize_cols, chip_resize_rows),\n",
        "                              preserve_range=True,\n",
        "                              anti_aliasing=True).astype(np.float16)\n",
        "\n",
        "            yield (origin_x, origin_y, chip, original_shape)\n",
        "\n",
        "\n",
        "def get_window(matrix, x, y, width, height):\n",
        "    return matrix[y:y + height, x:x + width]\n",
        "\n",
        "\n",
        "def set_window(matrix, x, y, new_matrix):\n",
        "    for i_index, i in enumerate(range(y, y + new_matrix.shape[0])):\n",
        "        for j_index, j in enumerate(range(x, x + new_matrix.shape[1])):\n",
        "            matrix[i][j] = new_matrix[i_index][j_index]\n",
        "\n",
        "\n",
        "def get_grids(grids, chip_size):\n",
        "    # multiplying chip_size by 1.5 to prevent missing data in rotation\n",
        "    grids_dict = {\n",
        "        1: [\n",
        "            {\"steps\": (chip_size, chip_size),\n",
        "             \"chip_size\": (chip_size, chip_size)}\n",
        "        ],\n",
        "        2: [\n",
        "            {\"steps\": (int(chip_size * 0.5), int(chip_size * 0.5)),\n",
        "             \"chip_size\": (chip_size, chip_size)},\n",
        "        ],\n",
        "        3: [\n",
        "            {\"steps\": (int(chip_size * 0.9), int(chip_size * 0.9)),\n",
        "             \"chip_size\": (chip_size, chip_size)},\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    return grids_dict[grids]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbVIhe7Z6Ty7"
      },
      "source": [
        "### U-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9uL77bj08t6",
        "outputId": "c7c073f8-c5ce-423f-f3d5-c0d4fbba6718"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Conv2D, Dropout, MaxPooling2D, Conv2DTranspose, concatenate, BatchNormalization, Activation\n",
        "\n",
        "\n",
        "def conv(n_filters, kernel_size, activation='elu', inputs=None):\n",
        "    net = Conv2D(n_filters, kernel_size, activation=None, kernel_initializer='he_normal', padding='same') (inputs) \n",
        "    net = BatchNormalization()(net)\n",
        "    net = Activation(activation)(net)\n",
        "    return net\n",
        "\n",
        "def transpose(n_filters, kernel_size, activation='elu', inputs=None):\n",
        "    net = Conv2DTranspose(n_filters, kernel_size, activation=None, strides=(2, 2), padding='same', kernel_initializer='he_normal') (inputs)\n",
        "    net = BatchNormalization()(net)\n",
        "    net = Activation(activation)(net)\n",
        "    return net\n",
        "\n",
        "def model_fn(input_shape, n_filters=64):\n",
        "    inputs = keras.Input(input_shape)\n",
        "\n",
        "    c1 = conv(n_filters * 1, (3, 3), inputs=inputs)\n",
        "    c1 = conv(n_filters * 1, (3, 3), inputs=c1)\n",
        "    p1 = MaxPooling2D((2, 2)) (c1)\n",
        "    p1 = Dropout(0.25) (p1)\n",
        "\n",
        "    c2 = conv(n_filters * 2, (3, 3), inputs=p1)\n",
        "    c2 = conv(n_filters * 2, (3, 3), inputs=c2)\n",
        "    p2 = MaxPooling2D((2, 2)) (c2)\n",
        "    p2 = Dropout(0.25) (p2)\n",
        "\n",
        "    c3 = conv(n_filters * 4, (3, 3), inputs=p2)\n",
        "    c3 = conv(n_filters * 4, (3, 3), inputs=c3)\n",
        "    p3 = MaxPooling2D((2, 2)) (c3)\n",
        "    p3 = Dropout(0.5) (p3)\n",
        "\n",
        "    c4 = conv(n_filters * 8, (3, 3), inputs=p3)\n",
        "    c4 = conv(n_filters * 8, (3, 3), inputs=c4)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
        "    p4 = Dropout(0.5) (p4)\n",
        "\n",
        "    c5 = conv(n_filters * 16, (3, 3), inputs=p4)\n",
        "    c5 = conv(n_filters * 16, (3, 3), inputs=c5)\n",
        "    c5 = Dropout(0.5) (c5)\n",
        "\n",
        "    u6 = transpose(n_filters * 8, (2, 2), inputs=c5)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    c6 = conv(n_filters * 8, (2, 2), inputs=u6)\n",
        "    c6 = conv(n_filters * 8, (2, 2), inputs=c6)\n",
        "    c6 = Dropout(0.5) (c6)\n",
        "    \n",
        "    u7 = transpose(n_filters * 4, (2, 2), inputs=c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    c7 = conv(n_filters * 4, (2, 2), inputs=u7)\n",
        "    c7 = conv(n_filters * 4, (2, 2), inputs=c7)\n",
        "    c7 = Dropout(0.5) (c7)\n",
        "\n",
        "    u8 = transpose(n_filters * 2, (2, 2), inputs=c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    c8 = conv(n_filters * 2, (2, 2), inputs=u8)\n",
        "    c8 = conv(n_filters * 2, (2, 2), inputs=c8)\n",
        "    c8 = Dropout(0.25) (c8)\n",
        "    \n",
        "    u9 = transpose(n_filters * 1, (2, 2), inputs=c8)\n",
        "    u9 = concatenate([u9, c1], axis=3)\n",
        "    c9 = conv(n_filters * 1, (2, 2), inputs=u9)\n",
        "    c9 = conv(n_filters * 1, (2, 2), inputs=c9)\n",
        "    c9 = Dropout(0.25) (c9)\n",
        "\n",
        "    outputs = conv(1, (1, 1), activation='sigmoid', inputs=c9)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # começa o treinamento com 0.01, depois de perceber que estáá começando a ficar instáável, \n",
        "    # altera para 0.0001\n",
        "    optimizer = tf.keras.optimizers.Nadam(learning_rate=0.0001) # 0.01 or 0.0001\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[iou])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLv-sqb36nuV"
      },
      "source": [
        "## Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngmo5JVC2DzF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from osgeo import gdal\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Classifier(object):\n",
        "    def __init__(self, model, model_dir):\n",
        "        self.__model = model\n",
        "        self.__model.summary()\n",
        "\n",
        "        checkpoint_path = \"{dir}/model.ckpt\".format(dir=model_dir)\n",
        "\n",
        "        latest = tf.train.latest_checkpoint(model_dir)\n",
        "\n",
        "        if latest:\n",
        "            model.load_weights(latest)\n",
        "\n",
        "        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=checkpoint_path,\n",
        "            save_weights_only=True,\n",
        "            save_best_only=True)\n",
        "\n",
        "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=model_dir)\n",
        "\n",
        "        self.__callbacks = [cp_callback, tensorboard_callback]\n",
        "\n",
        "    def train(self, input_train, input_validation, epochs, batch_size):\n",
        "        train_file, train_data, train_labels = load_dataset(input_train, read_only=True)\n",
        "        validation_file, validation_data, validation_labels = load_dataset(input_validation, read_only=True)\n",
        "\n",
        "        train_images = np.asarray(train_data[:8000], dtype=np.float16)\n",
        "        train_labels = np.asarray(train_labels[:8000], dtype=np.int8)\n",
        "        \n",
        "        validation_images = np.asarray(validation_data[:2000], dtype=np.float16)\n",
        "        validation_labels = np.asarray(validation_labels[:2000], dtype=np.int8)\n",
        "        \n",
        "        print(\"\\nTrain: {0}\\n\".format(train_images.shape[0]))\n",
        "        print(\"Validation: {0}\\n\".format(validation_images.shape[0]))\n",
        "\n",
        "        return self.__model.fit(x=train_images, \n",
        "                         y=train_labels,\n",
        "                         validation_data=(validation_images, validation_labels),\n",
        "                         epochs=epochs,\n",
        "                         batch_size=batch_size,\n",
        "                         verbose=1,\n",
        "                         callbacks=self.__callbacks,\n",
        "                         shuffle=True)\n",
        "      \n",
        "\n",
        "    \n",
        "\n",
        "    def evaluate(self, input_test, batch_size):\n",
        "        test_file, test_data, test_labels = load_dataset(input_test, read_only=True)\n",
        "\n",
        "        test_data = np.asarray(test_data, dtype=np.float16)\n",
        "        test_labels = np.asarray(test_labels, dtype=np.int8)\n",
        "\n",
        "        test_results = self.__model.evaluate(test_data,\n",
        "                                             test_labels,\n",
        "                                             batch_size=batch_size)\n",
        "        \n",
        "        print('test loss, test acc:', test_results)\n",
        "\n",
        "    def predict(self, input_path, output_path, chip_size, channels, grids, batch_size):\n",
        "\n",
        "        input_dataset = gdal.Open(input_path)\n",
        "\n",
        "        image = load_file(input_path, norm=True)[:, :, :channels]\n",
        "\n",
        "        predicted_image = np.zeros((image.shape[0], image.shape[1]), dtype=np.int8)\n",
        "\n",
        "        grids = get_grids(grids, chip_size)\n",
        "\n",
        "        for step in grids:\n",
        "            batch = []\n",
        "            windows = sliding_window(image, step[\"steps\"], step[\"chip_size\"],\n",
        "                (chip_size, chip_size))\n",
        "\n",
        "            for (x, y, chip, original_dimensions) in tqdm(iterable=windows,\n",
        "                                                          miniters=10,\n",
        "                                                          unit=\" windows\"):\n",
        "\n",
        "                normalized_chip = normalize(chip)\n",
        "\n",
        "         \n",
        "                batch.append({\n",
        "                    \"chip\": normalized_chip,\n",
        "                    \"x\": x,\n",
        "                    \"y\": y,\n",
        "                    \"dimensions\": original_dimensions\n",
        "                })\n",
        "\n",
        "                if len(batch) >= batch_size:\n",
        "                    chips = []\n",
        "                    positions = []\n",
        "                    dimensions = []\n",
        "\n",
        "                    for b in batch:\n",
        "                        chips.append(b.get(\"chip\"))\n",
        "                        positions.append((b.get(\"x\"), b.get(\"y\")))\n",
        "                        dimensions.append(b.get(\"dimensions\"))\n",
        "\n",
        "                    chips = np.array(chips, dtype=np.float16)\n",
        "\n",
        "                    pred = self.__model.predict(chips)\n",
        "\n",
        "                    for chip, position, dimension, predict in zip(chips,\n",
        "                                                                  positions,\n",
        "                                                                  dimensions,\n",
        "                                                                  pred):\n",
        "                        predict[predict > 0.5] = 1\n",
        "                        predict[predict <= 0.5] = 0\n",
        "\n",
        "                        predict = resize(predict, (dimension[0], dimension[1]),\n",
        "                            preserve_range=True,\n",
        "                            anti_aliasing=True).astype(np.int8)\n",
        "\n",
        "                        predict = predict.reshape(\n",
        "                            (predict.shape[0], predict.shape[1]))\n",
        "\n",
        "                        predicted = get_window(predicted_image,\n",
        "                                               position[0],\n",
        "                                               position[1],\n",
        "                                               predict.shape[1],\n",
        "                                               predict.shape[0])\n",
        "\n",
        "                        if predict.shape != predicted.shape:\n",
        "                            raise Exception(\"predict.shape != predicted.shape\")\n",
        "\n",
        "                        set_window(predicted_image,\n",
        "                                   position[0],\n",
        "                                   position[1],\n",
        "                                   np.add(predict, predicted))\n",
        "                    batch = []\n",
        "            print(\"Saving results...\")\n",
        "            driver = input_dataset.GetDriver()\n",
        "            output_dataset = driver.Create(output_path,\n",
        "                                           image.shape[1],\n",
        "                                           image.shape[0],\n",
        "                                           1,\n",
        "                                           gdal.GDT_Int16,\n",
        "                                           ['COMPRESS=DEFLATE'])\n",
        "            output_dataset.SetGeoTransform(input_dataset.GetGeoTransform())\n",
        "            output_dataset.SetProjection(input_dataset.GetProjection())\n",
        "            output_dataset.GetRasterBand(1) \\\n",
        "                .WriteArray(predicted_image.reshape((predicted_image.shape[0],\n",
        "                                                     predicted_image.shape[1])\n",
        "                                                    ), 0, 0)\n",
        "            output_dataset.FlushCache()\n",
        "            output_dataset = None\n",
        "            print(\"The results have been saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvBeBHcr6sIO"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k13p8yZkSVIR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from os import listdir\n",
        "from os.path import isfile, join, sep, exists\n",
        "\n",
        "def iou(y_true, y_pred):\n",
        "    y_pred = K.cast(K.greater(y_pred, .5), dtype='float32') # .5 is the threshold\n",
        "    inter = K.sum(K.sum(K.squeeze(y_true * y_pred, axis=3), axis=2), axis=1)\n",
        "    union = K.sum(K.sum(K.squeeze(y_true + y_pred, axis=3), axis=2), axis=1) - inter\n",
        "    acc = K.mean((inter + K.epsilon()) / (union + K.epsilon()))\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKayqlEx6xVs"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzj56TaSy_W5",
        "outputId": "da87ad5f-aa84-453e-b67d-0a14eaf935f9"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join, sep, exists\n",
        "\n",
        "model = model_fn((PREDICT_CHIP_SIZE, PREDICT_CHIP_SIZE, CHANNELS))\n",
        "\n",
        "classifier = Classifier(model=model, model_dir=MODEL_DIR)\n",
        "\n",
        "if not os.path.exists(PREDICT_INPUT_DIR):\n",
        "    print(\"Please wait until Google Earth Engine finishes processing your task.\")\n",
        "\n",
        "files = [f for f in listdir(PREDICT_INPUT_DIR) if isfile(join(PREDICT_INPUT_DIR, f))]\n",
        "\n",
        "if len(files) == 0:\n",
        "    print(\"No file found.\")\n",
        "\n",
        "for f in files:\n",
        "    print(\"File:\", f)\n",
        "    input_file = \"{directory}{sep}{filepath}\".format(directory=PREDICT_INPUT_DIR,\n",
        "                                                     sep=sep,\n",
        "                                                     filepath=f)\n",
        "\n",
        "    output_file = \"{directory}{sep}{filepath}\".format(directory=PREDICT_OUPUT_DIR,\n",
        "                                                      sep=sep,\n",
        "                                                      filepath=f)\n",
        "\n",
        "    if not os.path.exists(PREDICT_OUPUT_DIR):\n",
        "            os.makedirs(PREDICT_OUPUT_DIR)\n",
        "\n",
        "    if exists(output_file):\n",
        "        print(\"File {} exists.\".format(output_file))\n",
        "        continue\n",
        "        \n",
        "    print(\"Predict: \", input_file, \"  >>  \", output_file)\n",
        "\n",
        "    classifier.predict(\n",
        "        input_path=input_file,\n",
        "        output_path=output_file,\n",
        "        chip_size=PREDICT_CHIP_SIZE,\n",
        "        channels=CHANNELS,\n",
        "        grids=PREDICT_GRIDS,\n",
        "        batch_size=PREDICT_BATCH_SIZE\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oCSIWu963Mg"
      },
      "source": [
        "# Create Merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUprkASbcjeP",
        "outputId": "709e9c02-8044-4e20-80e0-595943866b34"
      },
      "outputs": [],
      "source": [
        "!cd classifications && gdalbuildvrt tmp.vrt *_2022.tiff -hidenodata -srcnodata 0\n",
        "!cd classifications && GDAL_CACHEMAX=512 gdal_translate -of GTiff -a_nodata 0 -co COMPRESS=DEFLATE -co NUM_THREADS=ALL_CPUS -co BIGTIFF=YES  tmp.vrt merged_2022_2_tiles.tiff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiEPN5rg7BYL"
      },
      "source": [
        "## Export merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "qS1QW4GZrvGH",
        "outputId": "65f63e1a-29a8-4c9f-b8b3-00f54175f944"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_48443d09-98af-4830-b9d9-be36f3a0af71\", \"merged_2022_2_tiles.tiff\", 175850)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('/content/classifications/merged_2022_2_tiles.tiff')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to delete your results from colabs temporary storage, uncomment and run the cell bellow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1iiC9ludbHV",
        "outputId": "9ffbce68-f6ba-4287-d45b-be49292ea9ef"
      },
      "outputs": [],
      "source": [
        "#os.system(\"rm -r /content/classifications\")\n",
        "#os.system(\"rm -r /content/results\")\n",
        "#os.system(\"mkdir /content/classifications\")\n",
        "#os.system(\"mkdir /content/results\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "tqZVT2cI_tT7"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
